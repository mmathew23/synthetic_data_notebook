{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data = Simulated Data\n",
    "Synthetic data refers to generated data vs naturally occurring data or  \n",
    "data collected by people.\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Why Is It Useful?\n",
    "- **Cost-effective Data Collection:** Generating synthetic data is often  \n",
    "much cheaper than employing human annotators, making it an  \n",
    "economical choice for data collection.\n",
    "\n",
    "- **Overcoming Data Scarcity:** In domains where adequate real-world data  \n",
    "is scarce or difficult to collect, synthetic data can fill the gap, enabling  \n",
    "the training of machine learning models.\n",
    "\n",
    "- **Large Diverse Data:** Synthetic data can capture a wider distribution  \n",
    "than human collectors, and can produce the data at scale.\n",
    "\n",
    "- **Privacy Compliance:** It helps in scenarios where using real data could  \n",
    "violate privacy regulations. Synthetic data can be used to create anonymized  \n",
    "datasets that mimic real data characteristics without exposing sensitive  \n",
    "information.\n",
    "\n",
    "\n",
    "### Papers\n",
    "**Self-Instruct: Aligning Language Models with Self-Generated Instructions**  \n",
    "\n",
    "\n",
    "Finetuned a base GPT-3 on synthetic and noisy data which compares favorably  \n",
    "to InstructGPT (precursor to ChatGPT)\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor**  \n",
    "\n",
    "\n",
    "Generates synthetic data from GPT-3.5 to finetune a smaller model, T5\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Textbooks Are All You Need II (phi-1.5; Microsoft)**  \n",
    "\n",
    "\n",
    "Trained a model on 20B synthetic textbook quality tokens\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Rephrasing the Web: A Recipe for Compute and Data Efficient Language Modeling**  \n",
    "\n",
    "\n",
    "Rephrased documents via different prompt strategies, trained with a mix of  \n",
    "synthetic and natural data, and showed 3x improvement in training speed\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Self-Alignment with Instruction Backtranslation**  \n",
    "\n",
    "\n",
    "Proposes a scaleable process to self improve a model starting with a small  \n",
    "set a seed data, and iteratively generating synthetic data and finetuning\n",
    "\n",
    "\n",
    "\n",
    "## Data quality is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from enum import Enum\n",
    "from typing import Iterable, Optional, List, Literal, Union\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import asyncio\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY='sk-' + 'your api key'\n",
    "client = instructor.patch(AsyncOpenAI())\n",
    "MODEL='gpt-4-turbo-preview'\n",
    "\n",
    "semantic_text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatRole(str, Enum):\n",
    "    user: str = 'user'\n",
    "    system: str = 'system'\n",
    "    chat: str = 'chat'\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[ChatRole.user, ChatRole.system]\n",
    "    content: str\n",
    "\n",
    "class ChatSequence(BaseModel):\n",
    "    messages: List[Message] = Field(..., description=\"A list of chat turns in sequence\")\n",
    "\n",
    "def create_chat_instruction(system_instruction, user_instruction):\n",
    "    return ChatSequence(messages=[\n",
    "        Message(role=ChatRole.system, content=system_instruction),\n",
    "        Message(role=ChatRole.user, content=user_instruction)\n",
    "    ])\n",
    "\n",
    "def pretty_print(instance: BaseModel):\n",
    "    print(json.dumps(instance.dict(), indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Synthetic Data for RAG Embedding Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context(BaseModel):\n",
    "    body: str = Field(..., description=\"The text body of the context chunk\")\n",
    "    id: Optional[int] = Field(..., description=\"Number identifier of the context chunk\")\n",
    "    title: Optional[str] = Field(None, description=\"The tital of the context chunk\")\n",
    "\n",
    "\n",
    "class ContextQuestion(BaseModel):\n",
    "    question: str = Field(..., description=\"A question where the context is the answer.\")\n",
    "    question_type: Optional[str] = Field(None, description=\"The type of question\")\n",
    "    question_level: Optional[str] = Field(None, description=\"How difficult the question is\")\n",
    "\n",
    "class ContextQuestions(BaseModel):\n",
    "    questions: List[str] = Field(..., description=\"A sequence of questions where the context is the answer list should be at least 5 questions.\")\n",
    "    question_type: Optional[str] = Field(..., description=\"The type of questions indicated by user instruction\")\n",
    "    question_level: Optional[str] = Field(..., description=\"How difficult the question is\")\n",
    "\n",
    "def with_chain_of_thought(model: type[BaseModel], action: str) -> type[BaseModel]:\n",
    "    # Retrieve the original model's fields and types\n",
    "    # fields = {name: (field.outer_type_, ...) for name, field in model.__fields__.items()}\n",
    "    fields = {name: (field.annotation, field.default) for name, field in model.__fields__.items()}\n",
    "    \n",
    "    # Add the 'reasoning' field\n",
    "    fields['reasoning'] = (str, Field(..., description=f\"Let's think step by step to {action}\"))\n",
    "    \n",
    "    # Create the new model with '_ChainOfThought' appended to the original model name\n",
    "    new_model_name = f\"{model.__qualname__}_ChainOfThought\"\n",
    "    new_model = create_model(new_model_name, **fields)\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "CoTContextQuestion = with_chain_of_thought(ContextQuestion, action=\"Generate unique questions for the context\")\n",
    "CoTContextQuestions = with_chain_of_thought(ContextQuestions, action=\"Generate unique questions for the context\")\n",
    "\n",
    "\n",
    "class RagQuestionContext(BaseModel):\n",
    "    context: Context = Field(..., description=\"A sequence of conext chunks\")\n",
    "    questions: Union[ContextQuestion, ContextQuestions, CoTContextQuestion, CoTContextQuestions] = Field(..., description=\"A sequence of questions where the context is the answer.\")\n",
    "\n",
    "class IterableRagQuestionContext(BaseModel):\n",
    "    question_groups: List[RagQuestionContext] = Field(..., description=\"A sequence of conext chunks and questions\")\n",
    "\n",
    "\n",
    "async def generate_questions(answers: List[Context], response_model=ContextQuestion) -> IterableRagQuestionContext:\n",
    "    results = []\n",
    "    for answer in answers:\n",
    "        # format context\n",
    "        context = answer.body\n",
    "        instructions = create_chat_instruction(\n",
    "            system_instruction=\"You are a helpful assistant and a worldclass question master capable of generating questions of all skill level for a given answer.\",\n",
    "            user_instruction=f\"Create unique questions that the context contains the answer to. \\n\\nContext: \\n{context} \"\n",
    "\n",
    "        )\n",
    "        # TODO check temp and other params\n",
    "        questions = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=instructions.dict()['messages'],\n",
    "            response_model=response_model,\n",
    "        )\n",
    "        results.append(questions)\n",
    "    results = await asyncio.gather(*results)\n",
    "    results = IterableRagQuestionContext(question_groups=[RagQuestionContext(context=answers[i], questions=results[i]) for i in range(len(answers))])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Context(body='\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.', id=0, title=None),\n",
       " Context(body='Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.', id=1, title=None),\n",
       " Context(body='We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.', id=2, title=None),\n",
       " Context(body='Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n', id=3, title=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader('https://openai.com/research/video-generation-models-as-world-simulators')\n",
    "loader.requests_kwargs = {'verify':False}\n",
    "data = loader.load()\n",
    "# chunk it up\n",
    "answers = semantic_text_splitter.create_documents([data[0].page_content])\n",
    "answers = [Context(id=i, body=answer.page_content) for i, answer in enumerate(answers)]\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Ask For It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question_groups\": [\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What transformative approach did Sora apply to the process of generating videos and images that enabled it to produce content with diverse durations, aspect ratios, and resolutions?\",\n",
      "        \"question_type\": \"Specific\",\n",
      "        \"question_level\": \"Medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What method can be used to extend a video both forward and backward in time to create a seamless infinite loop?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": \"Easy\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What technique is used to generate high-resolution images in the given context?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": \"easy\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"Which model demonstrates promise for the development of capable simulators for the physical and digital world?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": \"Easy\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "questions = await generate_questions(answers)\n",
    "pretty_print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Chain of Thought Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question_groups\": [\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"In what context is Sora utilized as a tool for creative outputs?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"Sora, as described in the context, is a video generation model that serves multiple creative functions. It is exploited for generating high fidelity videos of variable durations, resolutions, and aspect ratios, suitable for different devices and use cases. This capability enables the creation of content directly in native aspect ratios for various platforms, allows for the quick prototyping of content at lower sizes before generating at full resolution, and improves video framing and composition. Sora also facilitates tasks such as creating perfectly looping video, animating static images, extending videos forwards or backwards in time, and generating videos based on text prompts or pre-existing images or video inputs. Through the example of animating DALL\\u00b7E images and extending videos, it is clear that Sora aids in a wide array of creative tasks, making it an advanced tool for content creation and video editing. Thus, the context involves using Sora within creative and content production processes, leveraging its advanced generative capabilities for video and animation tasks.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What technique is used to transform the styles and environments of input videos zero-shot?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"Within the context, a technique named SDEdit is mentioned as being applied to Sora for transforming the styles and environments of input videos in a zero-shot fashion. This indicates the technique used is SDEdit.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What is the maximum image resolution that the model can generate?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"Examining the context provided, it is mentioned that the model can generate images of variable sizes, with the maximum resolution being up to 2048x2048. This detail provides the specific answer to the question about the model's maximum image resolution capability.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"question\": \"What are some common failure modes observed in video models according to the description?\",\n",
      "        \"question_type\": null,\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"The context mentions that aside from interactions like eating food not always leading to correct changes in object state, there are other failure modes of the model listed on the landing page. These include incoherencies that develop in long-duration samples and spontaneous appearances of objects. So, these aspects are directly referred to as common failure modes.\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = await generate_questions(answers, response_model=CoTContextQuestion)\n",
    "pretty_print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While you're at it ask for many questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question_groups\": [\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What are the main capabilities of the Sora video generation model?\",\n",
      "          \"How does Sora handle various aspect ratios and resolutions during video generation?\",\n",
      "          \"What is the significance of using spacetime patches in Sora's architecture?\",\n",
      "          \"Why is it beneficial for Sora to train on data in its native size instead of resizing or cropping?\",\n",
      "          \"How is language-understanding integrated into Sora's text-to-video generation system?\",\n",
      "          \"In what ways can Sora be prompted to generate videos?\",\n",
      "          \"What are some examples of videos generated by Sora based on prompts involving DALL\\u00b7E images?\",\n",
      "          \"How does Sora extend videos in either forward or backward in time?\"\n",
      "        ],\n",
      "        \"question_type\": \"Comprehension\",\n",
      "        \"question_level\": \"Intermediate\",\n",
      "        \"reasoning\": \"These questions are tailored to gain an intermediate-level understanding of the Sora video generation model as mentioned in the provided context. It covers the model's main capabilities, architectural choices, and innovative techniques, such as handling diverse video properties, leveraging spacetime patches, and integrating language understanding for generating text-to-video content. The questions also inquire about input methods for the model and specific examples of its output, providing a comprehensive overview of Sora's functionalities and its advancements in the field of video generation and editing.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"How can video editing be used to create a seamless infinite loop?\",\n",
      "          \"What technique allows for the transformation of video styles and environments?\",\n",
      "          \"Name one application of this video editing technique.\",\n",
      "          \"What changes can SDEdit make to a video's setting or style?\",\n",
      "          \"How can videos be connected to create seamless transitions?\",\n",
      "          \"What else, besides videos, is Sora capable of generating?\"\n",
      "        ],\n",
      "        \"question_type\": \"context\",\n",
      "        \"question_level\": \"intermediate\",\n",
      "        \"reasoning\": \"The context describes a technique for extending videos both forward and backward to produce a seamless infinite loop, known as video-to-video editing. It specifically mentions the use of `SDEdit` for transforming styles and environments of videos, showcasing multiple examples of the changes that can be made (e.g., changing the setting, style, etc.). Additionally, it mentions the method of connecting videos by gradually interpolating between two input videos to create seamless transitions. Finally, it notes that Sora is capable of generating not just videos but also images.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What technique is used for generating high-resolution images?\",\n",
      "          \"Which example demonstrates the model's ability to handle detailed close-up photography?\",\n",
      "          \"What environment showcases the model's ability to generate vibrant, life-like scenes under water?\",\n",
      "          \"In what setting is a young tiger depicted, and what notable artistic style is mentioned?\",\n",
      "          \"Describe the scene involving a snowy mountain village. What makes it especially photorealistic?\",\n",
      "          \"What emerging capabilities have been observed in video models when trained extensively?\",\n",
      "          \"How does the model ensure 3D consistency in generated videos?\",\n",
      "          \"What challenge have video generation systems faced in terms of temporal consistency, and how does Sora address this?\",\n",
      "          \"Describe an example of how Sora interacts with the world in a simulated video.\",\n",
      "          \"How does Sora simulate digital worlds, specifically video games?\",\n",
      "          \"What are the limitations of Sora as a simulator?\",\n",
      "          \"What future potential does the scaling of video models have, according to the context?\"\n",
      "        ],\n",
      "        \"question_type\": \"contextual\",\n",
      "        \"question_level\": \"advanced\",\n",
      "        \"reasoning\": \"The context provides detailed information about the capabilities and limitations of a model named Sora, focusing on its ability to generate high-resolution images and videos that exhibit emergent properties such as 3D consistency, long-range coherence, object permanence, and the simulation of interactions within both the physical and digital worlds. The questions have been crafted to highlight different aspects and applications of Sora, ranging from its technical approach to generating images, to specific scenarios and environments it can simulate, its emerging capabilities, and existing limitations. Each question targets a unique detail or feature mentioned in the context, encouraging a deep dive into the specific capabilities and future potential of Sora as described.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What are the common failure modes of this model?\",\n",
      "          \"What does Sora's capabilities today indicate about the future of video model scaling?\",\n",
      "          \"Who are the authors of this context?\",\n",
      "          \"What should one use for citing this work?\",\n",
      "          \"What other projects or services are mentioned along with Sora?\",\n",
      "          \"Which platforms does OpenAI have a presence on, according to the context?\"\n",
      "        ],\n",
      "        \"question_type\": \"comprehension\",\n",
      "        \"question_level\": \"advanced\",\n",
      "        \"reasoning\": \"Each question is designed to target specific parts of the context. The first question looks into the technical details of the model's limitations. The second question seeks to understand the implications of Sora's current capabilities for future research directions. The inclusion of author names and citation information in the questions targets the academic integrity and sourcing aspect. Mention of other projects and OpenAI's social media platforms aims to broaden the scope of inquiry to the organization's broader ecosystem and its communication channels.\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "questions = await generate_questions(answers, response_model=CoTContextQuestions)\n",
    "pretty_print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Few Shot Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def few_shot_generate_questions(answers: List[Context], few_shot_examples: List[RagQuestionContext], response_model=ContextQuestion, question_types: List[str] = [None], question_levels: List[str] = [None]) -> IterableRagQuestionContext:\n",
    "    results = []\n",
    "    few_shot_context = ''\n",
    "    for example in few_shot_examples:\n",
    "        few_shot_context += f\"Context: {example.context.body}\\n\"\n",
    "        questions = ''\n",
    "        for q in example.questions:\n",
    "            questions += f\"Question: {q}\\n\"\n",
    "        few_shot_context += f\"Question: {questions}\\n\\n\"\n",
    "    \n",
    "    rate_limit_buffer = 20\n",
    "    gathered_results = []\n",
    "    full_results = []\n",
    "    completion_count = 0\n",
    "    for answer in answers:\n",
    "        for question_type in question_types:\n",
    "            if question_type:\n",
    "                topic = f\"The question type should be {question_type}. \"\n",
    "            else:\n",
    "                topic = ''\n",
    "            for question_level in question_levels:\n",
    "                if question_level:\n",
    "                    topic += f\"The question level should be {question_level}. \"\n",
    "                # format context\n",
    "                context = answer.body\n",
    "                instructions = create_chat_instruction(\n",
    "                    system_instruction=\"You are a helpful assistant and a worldclass question master capable of generating questions of all skill level for a given answer.\",\n",
    "                    user_instruction=f\"Create unique questions that the context contains the answer to. {topic} Follow the these examples and format:\\n\\n{few_shot_context} Context \\n\\nContext: \\n{context} \"\n",
    "\n",
    "                )\n",
    "                # TODO check temp and other params\n",
    "                questions = client.chat.completions.create(\n",
    "                    model=MODEL,\n",
    "                    messages=instructions.dict()['messages'],\n",
    "                    response_model=response_model,\n",
    "                )\n",
    "                results.append((answer, questions))\n",
    "                full_results.append((answer, questions))\n",
    "                completion_count += 1\n",
    "                if len(results) >= rate_limit_buffer:\n",
    "                    gathered_results.extend(await asyncio.gather(*list(zip(*results))[1]))\n",
    "                    results = []\n",
    "    gathered_results.extend(await asyncio.gather(*list(zip(*results))[1]))\n",
    "    # results keys and values are in the same order so we can map back\n",
    "    results = [(orig_tuple[0], gathered) for gathered, orig_tuple in zip(gathered_results, full_results)]\n",
    "    results = IterableRagQuestionContext(\n",
    "        question_groups=[RagQuestionContext(\n",
    "            context=context,\n",
    "            questions=question,\n",
    "        )\n",
    "        for context, question in results]\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [RagQuestionContext(\n",
    "    context=Context(id=40, body=\"We follow the methods of Yu et al. (2022a) and jointly pre-train our captioner with a CLIP and a language modeling objective using the above formulation on our dataset of (t, i) text and image pairs. The resulting model is indeed a good captioner, but exhibits the same problems we describe in section 2, such as a reluctance to describe details.\"),\n",
    "    questions=ContextQuestions(questions=[\"What objectives is the captioner trained with?\"], question_type=None, question_level=None)),\n",
    "    RagQuestionContext(context=Context(id=503, body=\"\"\"To evaluate caption blending ratios, we trained four image generation models using our descriptive synthetic captions at different blending ratios. We chose blends of 65%, 80%, 90% and 95% synthetic captions. Midway through the experiment, evaluations showed that the 65% blend was far behind the other blends on all evals and we dropped it.\"\"\"),\n",
    "                       questions=ContextQuestions(questions=[\"How can you evaluate caption blending ratios?\"], question_type=None, question_level=None)),\n",
    "    RagQuestionContext(context=Context(id=12, body=\"Annotating large-scale instruction data can be chal- lenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the solutions to each task. Here, we de- tail our process for SELF-INSTRUCT, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself, filtering the generated data, and then conducting instruction tuning with this generated data in order to align the LM to follow instructions better.\"),\n",
    "                       questions=ContextQuestions(questions=['Why is annotating instruction data challenging, particularly at large scale?'], question_type=None, question_level=None))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_groups': [{'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How do generative video models like Sora handle diverse formats of visual data?',\n",
       "     'What advantages do spacetime patches offer in video generation modeling?',\n",
       "     'How does Sora differentiate from past generative video models?',\n",
       "     'What benefits are seen by training video generation models like Sora on videos in their native sizes?',\n",
       "     'How does Sora utilize language understanding in video generation?',\n",
       "     'What capabilities does Sora provide when prompted with images or videos?',\n",
       "     'In what ways can Sora extend the functionality of generated videos?'],\n",
       "    'question_type': None,\n",
       "    'question_level': None,\n",
       "    'reasoning': \"The context discusses the technical aspects and capabilities of Sora, a large-scale generative model for video generation. Key points include the transformation of videos into unified representations using spacetime patches, the scaling of transformers in video generation, training on native sizes for flexibility and better framing, incorporating language understanding for improved fidelity, and the model's ability to be prompted with and modify existing images or videos as well as extend videos in time. Questions are designed to probe the reader's understanding of these aspects and how they contribute to the model's utility as a world simulator.\"}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How can videos be seamlessly looped using SDEdit?',\n",
       "     'What does SDEdit enable for video-to-video editing?',\n",
       "     'How can you transform the environment of a video using text prompts?',\n",
       "     'What are some examples of environmental transformations SDEdit can apply to videos?',\n",
       "     'How can two different videos be connected using Sora?',\n",
       "     'What capabilities does Sora have in terms of image generation?'],\n",
       "    'question_type': None,\n",
       "    'question_level': None,\n",
       "    'reasoning': 'The context describes a method of creating seamless loops in videos by extending them backward (or forward) in time using a generation technique. SDEdit is introduced as a technique enabling zero-shot transformations of styles and environments in videos through text prompts. It mentions various transformation examples, such as changing settings to different environments (e.g., lush jungle, old school, underwater, etc.), and explains the process of connecting two videos to achieve a seamless transition between them using Sora. Additionally, the capabilities of Sora in generating images are mentioned, showing the broad potential of these technologies in video and image editing.'}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are some emergent capabilities of Sora when trained at scale?',\n",
       "     'How does Sora maintain temporal consistency in video generation?',\n",
       "     'What limitations does Sora have as a simulator?',\n",
       "     'How can Sora simulate actions that affect the state of the world?',\n",
       "     'In what way can Sora simulate digital worlds, and provide an example?',\n",
       "     'Why is continuing to scale video models considered promising?'],\n",
       "    'question_type': None,\n",
       "    'question_level': None,\n",
       "    'reasoning': \"The context describes Sora, a model trained to simulate video content with dynamic capabilities such as 3D consistency, long-range coherence, object permanence, and interaction with the world. It also mentions Sora's ability to simulate actions affecting the world, like leaving bite marks on a burger. Additionally, Sora can simulate digital worlds, exemplified by controlling a player in Minecraft. The limitations of Sora include an inaccurate modeling of basic physics interactions, like glass shattering. The context suggests that scaling video models is a promising path towards more capable simulators, due to the emergent capabilities of models like Sora.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are some common failure modes of the model mentioned?',\n",
       "     'What do the authors believe about the scaling of video models?',\n",
       "     \"Who is part of the team that authored the document discussing Sora's capabilities?\"],\n",
       "    'question_type': None,\n",
       "    'question_level': None,\n",
       "    'reasoning': \"Given the context, the questions are designed to extract specific information outlined in the text. The first question seeks information on the model's shortcomings or failure modes as mentioned. The second question targets the authors' beliefs regarding the future of video model scaling and its implications for simulating the physical and digital worlds. The last question aims to identify the team members involved in the research, making it relevant to someone interested in the contributions or credentials of individuals who worked on the project.\"}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = await few_shot_generate_questions(answers, few_shot_examples, response_model=CoTContextQuestions)\n",
    "questions.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steer The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_groups': [{'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What enables Sora to generate high-quality videos that follow user prompts?',\n",
       "     'How do video generation models like Sora benefit from training on data in its native aspect ratios?',\n",
       "     'What unique capabilities does Sora demonstrate when prompted with images and videos?',\n",
       "     'In what ways is Sora compared to previous generative models?',\n",
       "     'What technological approaches are used to transform videos into a manageable format for Sora?',\n",
       "     \"How does Sora's training approach differ from traditional methods?\",\n",
       "     'Can Sora extend the duration of videos, and if so, how?',\n",
       "     'What innovations does Sora contribute to video generation technology?',\n",
       "     \"Why is the transformer architecture significant for Sora's functionality?\",\n",
       "     'How does Sora improve upon framing and composition in video content?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'null',\n",
       "    'reasoning': \"The context outlines the capabilities and technological innovations of Sora, a video generation model. The questions extract information on Sora's training methods, benefits of training on data in its native size, and its unique ability to understand language and prompts. The questions also delve into the technical specifics of transforming videos into patches, the significance of the transformer architecture, and comparisons with previous models. These questions require understanding and interpreting the technical descriptions provided in the context to uncover the underlying advancements and functionalities of Sora.\"}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How do video generation models like Sora handle diverse video and image types in their training process?',\n",
       "     'What architectural elements enable Sora to generate high fidelity video?',\n",
       "     'Which models have been foundational in the study of generative video modeling?',\n",
       "     'How does Sora achieve training on videos of variable durations, resolutions, and aspect ratios?',\n",
       "     'What are the benefits of training video generation models on data at its native size?',\n",
       "     'How does Sora enhance language understanding in video generation?',\n",
       "     'In what ways can Sora be prompted to generate videos?',\n",
       "     'How does Sora utilize diffusion transformers in video generation?',\n",
       "     'What improvements does Sora offer over models trained on square-cropped videos?',\n",
       "     'How does utilizing detailed captions impact the video generation quality of Sora?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'intermediate',\n",
       "    'reasoning': 'The context describes the technical aspects of Sora, a sophisticated video generation model capable of creating high-fidelity videos. It discusses how Sora handles training on various types of visual data, its architecture, previous models in the field, and the benefits of its training approach. Additionally, it details how language understanding is integrated into video generation, the versatility of Sora’s prompting mechanisms, and specific architectural choices like diffusion transformers. Understanding these aspects requires questions that probe the methodological and technical underpinnings of Sora’s capabilities, enhancements over previous approaches, and the impact of certain training techniques on output quality.'}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How can videos be seamlessly looped using diffusion models?',\n",
       "     \"What are some examples of changes SDEdit can make to an input video's setting or style?\",\n",
       "     'How does Sora interpolate between two input videos?',\n",
       "     'What capabilities does Sora have in addition to video-to-video editing?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'None',\n",
       "    'reasoning': \"The context explains the technique of extending videos backward and forward to create an infinite loop, mentions diverse changes the SDEdit can apply to video settings and styles such as changing the environment, era, or animation style, describes Sora's ability to create seamless transitions between entirely different videos, and highlights Sora's image generation capabilities beyond editing videos.\"}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What technique enables Sora to transform the styles and environments of input videos?',\n",
       "     'How is it possible to create a seamless infinite loop with the described method?',\n",
       "     'What is the result of extending a video backward from a generated segment?',\n",
       "     'Can Sora interpolate between two videos with different subjects to create seamless transitions?',\n",
       "     'Is Sora capable of generating images as well as editing videos?',\n",
       "     'What changes can Sora apply to the video settings based on the given examples?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'standard',\n",
       "    'reasoning': \"The context presents interesting capabilities of a video editing tool named Sora, which can transform the styles and environments of videos, and also discusses a method to extend videos both forward and backward to achieve a seamless infinite loop. Additionally, it highlights the unique transformations that can be applied to videos, like changing settings to different environments or styles (e.g., lush jungle, cyberpunk), the ability to create seamless transitions between different videos, and Sora's capacity to generate images. Based on these details, questions are crafted to test the understanding of Sora's functions and the application of the described video editing method.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are the emergent capabilities of Sora when trained at scale?',\n",
       "     'What phenomena allow video models like Sora to exhibit emergent capabilities?',\n",
       "     'How does Sora simulate the physical and digital world, including its inhabitants?',\n",
       "     'What limitations does Sora have as a simulator?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'None',\n",
       "    'reasoning': \"The context outlines the capabilities of a model named Sora, especially when trained at scale, detailing the abilities it has developed such as simulating aspects of the physical and digital world. It also mentions the phenomena that enable these capabilities, which are not attributed to any explicit inductive biases but rather phenomena of scale. Specific examples of Sora's capabilities include 3D consistency, long-range coherence, object permanence, and interaction with the world, as well as simulating digital worlds like video games. Finally, the context acknowledges limitations in Sora's simulation abilities, such as inaccurately modeling certain interactions.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What enables Sora to simulate aspects of the physical world, including people, animals, and environments?',\n",
       "     'How does Sora handle 3D consistency in generated videos?',\n",
       "     'What challenge do video generation systems face that Sora often overcomes?',\n",
       "     'In what ways can Sora interact with the world in its simulations?',\n",
       "     'How does Sora simulate digital worlds, such as video games?',\n",
       "     'What limitations does Sora currently have as a simulator?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'advanced',\n",
       "    'reasoning': \"The context discusses the various emergent capabilities of the video model Sora, which include simulating aspects of the physical and digital world without explicit inductive biases for 3D, objects, etc., merely as phenomena of scale. These capabilities enable Sora to generate videos with dynamic camera motion maintaining 3D consistency, handle long-range coherence and object permanence by effectively modeling both short- and long-range dependencies, interact with the world by simulating actions that affect the state of the world (e.g., a painter leaving strokes on a canvas, a man eating a burger and leaving bite marks), and simulate digital worlds like video games by controlling a player in Minecraft while rendering the world and its dynamics in high fidelity. The context also outlines Sora's limitations, such as not accurately modeling the physics of many basic interactions like glass shattering.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What demonstrates the potential of continued scaling of video models for simulating complex environments?',\n",
       "     'What common model failure modes does the paper detail?',\n",
       "     'How does the work of Brooks, Peebles, et al. contribute to the development of simulators?',\n",
       "     'What are the challenges in achieving correct object state changes in interactions like eating food within simulations?',\n",
       "     \"In what ways does Sora's capabilities today hint at future progress in simulating the physical and digital world?\"],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'intermediate',\n",
       "    'reasoning': 'The context describes the capabilities of Sora, a video model, and its demonstration of potential in scaling to simulate physical and digital worlds. It mentions common failure modes such as incoherencies in long-duration samples and spontaneous appearances of objects, which are detailed on a landing page. The authors believe in the promise of scaling video models like Sora for capable simulators. The context lists out the research team, signaling their contribution through the study and development of such simulators. Challenges in achieving correct object state changes, especially in interactions like eating food, are acknowledged, indicating the complexities of accurately simulating real-world interactions. The capabilities of Sora are highlighted as evidence for future progress in simulation technologies.'}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What does the continued scaling of video models indicate for simulators of the physical and digital world?',\n",
       "     'Which model interactions do not always result in correct changes in object state?',\n",
       "     'What are some common failure modes enumerated for the model?',\n",
       "     'Who are some of the authors involved in the research on simulators of the physical and digital world through video models?'],\n",
       "    'question_type': 'closed',\n",
       "    'question_level': 'intermediate',\n",
       "    'reasoning': 'Given the context of the research and findings presented, questions were formulated to target the implications of scaling video models (predicting future simulator capabilities), the reliability of model interactions (specifically eating food), an overview of common failure modes (to check understanding of model limitations), and acknowledgment of the research contributors (to note authorship and recognition).'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = await few_shot_generate_questions(answers, few_shot_examples, response_model=CoTContextQuestions, question_types=['open-ended', 'closed-ended'])\n",
    "questions.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_groups': [{'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What is the main purpose of scaling video generation models?',\n",
       "     'How does Sora differ from previous generative models of video data?',\n",
       "     \"Why are videos and images turned into patches for Sora's training?\",\n",
       "     'How does Sora generate high fidelity videos?',\n",
       "     'How does training on videos of variable sizes benefit Sora?',\n",
       "     'In what ways can Sora be prompted to generate videos?',\n",
       "     \"What technique is used to improve language understanding for Sora's video generation?\",\n",
       "     'How is Sora capable of extending generated videos?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'The context discusses the development and capabilities of Sora, a video generation model capable of generating high fidelity videos from text and other prompts. The purpose of scaling video generation models is to build general purpose simulators of the physical world. Sora is distinguished from previous models by its ability to generate videos of diverse durations, aspect ratios, and resolutions. It turns videos and images into patches for training, enabling it to handle diverse visual data efficiently. The generation process involves converting raw video into a lower-dimensional latent space and then decompressing it during inference. Training on videos of their native sizes allows for flexibility in sampling and improves video framing and composition. Sora can be prompted both with text and visually, e.g., with images or pre-existing videos, broadening its application scope. Language understanding is enhanced through training on highly descriptive video captions. Lastly, Sora can extend videos both forwards and backwards in time, showcasing its versatility.'}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What unique representational approach does Sora take for training on diverse types of videos and images?',\n",
       "     \"How does Sora's training differ from past approaches to video generation?\",\n",
       "     'In what ways can Sora be prompted to generate videos?',\n",
       "     \"What impact does training on highly descriptive video captions have on Sora's generated videos?\",\n",
       "     'How does Sora improve the framing and composition of generated videos compared to models trained on square crops?',\n",
       "     \"What are some applications of Sora's ability to understand and generate videos based on text?\",\n",
       "     'How does the capability to generate and extend videos make Sora versatile in video editing tasks?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'medium',\n",
       "    'reasoning': \"The context introduces Sora, a large-scale video generation model with several innovative features. It transforms visual data into a unified representation using spacetime patches, enhancing generative model training on diverse video and image types. Unlike previous models that often standardize video size, Sora trains on data at its native size, allowing flexibility in video duration, resolution, and aspect ratio. It can generate videos based on text prompts or from pre-existing images and videos, applying highly descriptive captions for improved text fidelity and overall quality. Additionally, Sora's method improves framing and composition by training on videos at their natural aspect ratios and enables a range of video editing tasks, such as creating perfectly looping videos or extending videos in time. The questions explore these unique features and their implications for Sora's training approach, generative capabilities, and potential applications.\"}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How does Sora turn videos into a unified representation for training?',\n",
       "     'What advantages does training on data in its native size offer for video generation?',\n",
       "     \"How does Sora's training on descriptive video captions affect its performance?\",\n",
       "     'Explain how Sora can generate videos from images.',\n",
       "     'What makes the prompting capability of Sora versatile?',\n",
       "     'Describe the process and benefits of scaling transformers for video generation according to the text.',\n",
       "     'How does Sora compare to models trained on square cropped videos?',\n",
       "     'What additional inputs besides text can Sora be prompted with?',\n",
       "     'In what ways can Sora extend generated videos?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'medium',\n",
       "    'reasoning': \"These questions cover specific aspects of the technical approach, implementation, and capabilities of Sora, detailed in the context about its video generation modeling. They are meant to deepen understanding of Sora's innovative aspects, including its method for turning visual data into patches, the benefits of training on data at its native size, how training on descriptive captions enhances text fidelity and video quality, the flexibility in generating videos from images, the versatility in prompting with different inputs, and the process of scaling transformers for video generation. Furthermore, these questions explore comparisons with other models and provide insight into Sora's unique ability to extend videos.\"}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What essensial elements enable Sora to train on diverse types of videos and images?',\n",
       "     'How does Sora achieve high quality video generation?',\n",
       "     'What unique capability allows Sora to perform a variety of editing tasks?',\n",
       "     'What approach does Sora use to improve language understanding in video generation?',\n",
       "     'How does Sora handle variable video durations, resolutions, and aspect ratios?',\n",
       "     'In what ways can you prompt Sora to generate content?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context provides detailed information about the technical aspects and capabilities of Sora, a video generation model. It explains the process of turning videos into patches and how this method allows for handling diverse types of videos and images. The context highlights the innovative use of diffusion transformers for scaling and improving the quality of generated videos. Furthermore, it mentions the ability to prompt Sora with various inputs, including images and pre-existing videos, enabling a wide range of editing tasks. Also, it details the training on highly descriptive video captions to enhance language understanding and fidelity in video generation. Lastly, it describes Sora's flexibility in generating content for different device aspect ratios by training on native-size data, accommodating variable durations, resolutions, and aspect ratios.\"}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['Why do models like Sora turn videos into patches for training?',\n",
       "     \"What is the purpose of the video compression network in the context of Sora's training?\",\n",
       "     'How does Sora handle videos and images of variable durations, resolutions, and aspect ratios?',\n",
       "     'What advantages does training on videos in their native sizes provide for Sora?',\n",
       "     'What method does Sora use to improve language understanding for video generation?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'medium',\n",
       "    'reasoning': \"To understand the comprehensive training process and architecture of Sora for effective video generation. The questions focus on: \\n- The rationale behind the use of patches for video representation.\\n- The role of the video compression network in reducing the dimensionality of visual data.\\n- Sora's capability to process videos and images irrespective of their variable characteristics.\\n- The benefits of utilizing videos' original sizes during training.\\n- Techniques implemented by Sora to enhance the text fidelity and quality of generated videos through improved language understanding.\"}},\n",
       "  {'context': {'body': '\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\xa0Sora,\\xa0Milestone,\\xa0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\xa0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \"key\": \"characters\"\\n} wearing {\\n  \"key\": \"clothings\"\\n} taking a pleasant stroll in {\\n  \"key\": \"places\"\\n} during {\\n  \"key\": \"weathers\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL·E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 2[^31] and DALL·E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells “SORA”.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.',\n",
       "    'id': 0,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What enables Sora to train on videos and images of various resolutions, durations, and aspect ratios?',\n",
       "     'How does Sora improve the framing and composition of generated videos compared to square-cropped training?',\n",
       "     'What benefit does using highly descriptive video captions for training Sora have on the generated videos?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': None,\n",
       "    'reasoning': \"For the easy question, identifying the feature that allows Sora to handle diverse video and image types is fairly straightforward, given the context's emphasis on the generative model's adaptability. For the medium question, understanding that training on videos at native aspect ratios rather than square crops improves composition and framing requires a bit more insight into how different training data affects the model's output. Lastly, for the hard question, grasping that training with highly descriptive captions enhances the text fidelity and overall quality of the generated videos demands a deeper comprehension of the relationship between textual and visual data in generative models.\"}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What technique allows Sora to transform the styles and environments of videos?',\n",
       "     'How can a video be edited to seamlessly loop both forward and backward?',\n",
       "     'What can be achieved by using Sora to interpolate between two videos?',\n",
       "     'What kind of settings can Sora change in a video?',\n",
       "     'Is Sora capable of generating images, besides editing videos?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'The context mentions using SDEdit to transform the styles and environments of input videos, which answers the first question. For the second question, the context describes a method where videos are extended backward from a segment, making seamless infinite loops possible. The third question is addressed by noting that Sora can interpolate between two videos, creating seamless transitions. Various settings mentioned, like changing a video to be in a lush jungle or a 1920s setting, address the fourth question. Lastly, it is stated that Sora is also capable of generating images, answering the fifth question.'}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How can videos be seamlessly extended to create an infinite loop?',\n",
       "     'What enables Sora to transform the environments and styles of videos?',\n",
       "     'What are some examples of video settings changes enabled by SDEdit for Sora?',\n",
       "     'How does Sora achieve transitions between videos with different subjects?',\n",
       "     'What additional capability does Sora have besides editing videos?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context describes a technique for extending videos forward and backward to create an infinite loop, mentioning the role of diffusion models and a specific method called SDEdit in altering video environments and styles. Examples of such alterations include changing the video's setting to various imaginative environments. It also mentions Sora's ability to interpolate between two videos for smooth transitions and its capability to generate images, providing a comprehensive overview of Sora's video and image editing functionalities.\"}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What is the purpose of extending a video both forward and backward?',\n",
       "     'How does SDEdit transform the styles and environments of input videos?',\n",
       "     'What are some examples of how the video setting can be changed using SDEdit?',\n",
       "     'What does the technique of gradually interpolating between two input videos achieve?',\n",
       "     'How does extending a video in both directions benefit video-to-video editing and diffusion models?',\n",
       "     'In what ways can Sora modify the environment of a video?',\n",
       "     'Describe the process and benefit of connecting videos through gradual interpolation.',\n",
       "     \"Explain how the ability to generate images complements Sora's video editing capabilities.\"],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': None,\n",
       "    'reasoning': 'The context provides an overview of using diffusion models and techniques like SDEdit for video editing and alterations, particularly focusing on video extension, environment transformation, seamless loops, and blending video scenes. Questions of varying difficulty are derived to explore the principle and application of these methods, the specific changes achievable with Sora, and their underpinning technology or rationale.'}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What technique allows Sora to transform the style and environment of video inputs zero-shot?',\n",
       "     'By extending a video both forward and backward, what effect can be achieved?',\n",
       "     'What can Sora do aside from editing videos?',\n",
       "     'Can Sora gradually interpolate between two distinct videos to create seamless transitions?',\n",
       "     'What is the purpose of extending videos backwards from a specific segment?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'In the context, it is indicated that the technique SDEdit enables Sora to transform the styles and environments of input videos zero-shot. It also mentions that extending a video both forward and backward allows for the creation of a seamless infinite loop. Additionally, Sora has the capability not only for video-to-video editing but also for generating images. Sora can interpolate between two distinct videos to create seamless transitions. Extending videos backwards from a specific segment is part of a technique to produce a seamless infinite loop as mentioned.'}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['How do diffusion models contribute to video editing?',\n",
       "     'What technique allows for the transformation of video styles and environments zero-shot?',\n",
       "     'In what way can videos be extended to create a seamless infinite loop?',\n",
       "     'What is the result of extending videos backward in time from a generated segment?',\n",
       "     'How can videos gradually interpolate between two different scenes or subjects?',\n",
       "     'Can Sora generate images as well as edit videos?',\n",
       "     'What are some examples of settings Sora can apply to videos?',\n",
       "     'Is it possible to maintain specific elements, like color, while changing the video setting with Sora?',\n",
       "     'How does extending videos both forward and backward benefit video editing?',\n",
       "     'Can Sora transform videos into different animation styles or themes?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context provides a detailed example of how diffusion models, specifically the SDEdit technique, are applied in video editing to change styles and environments of videos zero-shot. It explains the capability to extend videos to create infinite loops and to interpolate between videos for seamless transitions. Additionally, it mentions Sora's ability to generate images, apply various settings and themes to videos, and maintain specific video elements like color during transformations. This makes the basis for creating questions that are easy to answer from the given context.\"}},\n",
       "  {'context': {'body': 'Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\xa0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.',\n",
       "    'id': 1,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What allows for the creation of an infinite video loop?',\n",
       "     'Which technique allows Sora to change the styles and environments of videos?',\n",
       "     'How does Sora create transitions between videos with different subjects and scenes?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context describes a method where videos are extended backwards in time from a generated segment, leading to an infinite loop. It also mentions the use of SDEdit to transform the styles and environments of input videos for Sora, and the ability of Sora to interpolate between two input videos to create seamless transitions. These points directly answer the questions proposed, which are made to assess the reader's comprehension of the key points in the text.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What emergent capabilities do video models trained at scale demonstrate?',\n",
       "     'How does the model ensure 3D consistency in generated videos?',\n",
       "     'What is the challenge in maintaining temporal consistency in video generation systems?',\n",
       "     'How can Sora simulate actions that affect the state of the world?',\n",
       "     'In what ways can Sora simulate digital worlds like video games?',\n",
       "     'What are the limitations of Sora as a simulator?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context details the emergence of capabilities in Sora, a video generation model, when it's trained at scale. It outlines specific capabilities like 3D consistency, long-range coherence and object permanence, the simulation of actions affecting the world, and the simulation of digital worlds like video games. Moreover, it mentions Sora's limitations, such as not accurately modeling the physics of basic interactions. Each question is aimed at extracting understanding on distinct aspects mentioned in the context, making them unique and relevant to the described features and limitations of Sora.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What capabilities does Sora exhibit when trained at scale?',\n",
       "     'How does Sora handle 3D consistency and object permanence in video generation?',\n",
       "     'In what ways can Sora interact with and simulate the world?',\n",
       "     'What limitations does Sora currently have as a simulator?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'medium',\n",
       "    'reasoning': \"Analyzing the context reveals Sora's capabilities when trained at scale, such as generating videos with dynamic camera movement, maintaining temporal consistency over long videos, simulating actions that affect the world, and recreating artificial processes like video games. Furthermore, the context details challenges such as accurately modeling the physics of interactions like glass shattering. Therefore, questions are designed to probe understanding of Sora's emergent capabilities at scale, its handling of 3D consistency and object permanence, its interaction with and simulation of the world, and its current limitations, offering a medium-level difficulty that requires comprehension and synthesis of the information provided.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What capabilities does the model Sora exhibit when trained at scale?',\n",
       "     \"What does the ability to maintain 3D consistency imply about Sora's simulation of videos?\",\n",
       "     'How does Sora simulate interactions with the world and give an example?',\n",
       "     'In what ways are video game worlds simulated by Sora, and provide an example?',\n",
       "     'What are the limitations of Sora as a simulator?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': None,\n",
       "    'reasoning': 'The context provides detailed descriptions of the emergent capabilities of the model Sora when it is trained at scale. These capabilities include maintaining 3D consistency, long-range coherence and object permanence, interacting with the world, and simulating digital worlds like video games. An example of Sora simulating interactions is a man eating a burger and leaving bite marks. In the context of simulating video games, Sora can control a player in Minecraft while rendering the world and its dynamics. However, the model has its limitations, such as not accurately modeling the physics of certain interactions like glass shattering.'}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What enables Sora to simulate certain aspects of the physical world?',\n",
       "     'What are some emergent capabilities of Sora when trained at scale?',\n",
       "     'Can Sora generate images with variable resolutions?',\n",
       "     'Is Sora capable of maintaining 3D consistency in videos?',\n",
       "     'Does Sora succeed in maintaining long-range coherence and object permanence in videos?',\n",
       "     'Can Sora simulate actions that change the world in simple ways, like eating or painting?',\n",
       "     'Is Sora able to simulate video games like Minecraft?',\n",
       "     'What are some limitations of Sora as a simulator?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'The context describes various capabilities and limitations of Sora, an advanced simulation model. It mentions emergent properties like 3D consistency, long-range coherence, and the ability to simulate interactions with the world and video games. These capabilities and limitations can directly form the basis of closed-ended questions that are considered easy because they require identifying specific details mentioned in the passage.'}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What enables Sora to simulate some aspects of the physical world?',\n",
       "     'In terms of video generation, what challenge has Sora managed to partially overcome?',\n",
       "     'How does Sora simulate interactions that change the state of the world?',\n",
       "     'What kind of digital processes can Sora simulate effectively?',\n",
       "     'What limitations does Sora currently face as a simulator?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'medium',\n",
       "    'reasoning': \"The context explains the capabilities and limitations of Sora in simulating videos that replicate both the physical and digital worlds. These explanations serve as the basis for developing questions around the specifics of its capabilities (like simulating aspects of the physical world, overcoming video generation challenges, simulating interactions with the world, and simulating digital processes) and its limitations. By focusing on these key areas, the questions assess the reader's understanding of Sora's functionalities and the current challenges it faces as a simulator.\"}},\n",
       "  {'context': {'body': 'We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning “Minecraft.”These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.',\n",
       "    'id': 2,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What resolution can the model generate images up to?',\n",
       "     'How does Sora maintain three-dimensional space consistency?',\n",
       "     'In what way does Sora simulate interactions that affect the state of the world?',\n",
       "     'What are some limitations of Sora as a simulator?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The answer to the first question about the maximum image resolution (2048x2048) is explicitly stated in the context, which should be easy for readers to find. The answer to the second question on maintaining three-dimensional space consistency through dynamic camera motion is also directly mentioned, making it straightforward. The third question queries how Sora simulates interactions like leaving bite marks or painting strokes, which is described in the context and should be easily understood. Lastly, the fourth question about Sora's limitations, such as not accurately modeling the physics of interactions like glass shattering, is addressed directly in the context, thus fitting the easy category.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What challenges do video models face in simulating the physical and digital world?',\n",
       "     'Who are the authors of the study on the development of simulators using video models?',\n",
       "     'What does the capability of Sora indicate about the future of video model scaling?',\n",
       "     'Where can the citation for this study be found?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'The context discusses the capabilities of a video model named Sora and the challenges it faces, such as incorrect changes in object state and spontaneous appearances of objects. It highlights the significance of scaling video models for simulating the physical and digital world. The authors of the study are listed, and the context provides a link for citation, making these elements relevant for generating questions that are easy and open-ended.'}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are some common failure modes of the model mentioned?',\n",
       "     'How does the capabilities of Sora contribute to the development of simulators?',\n",
       "     'What is the promising path towards developing capable simulators of the physical and digital world according to the authors?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The context mentions specific failure modes of the model, highlights the role of Sora's capabilities in advancing the development of simulators, and suggests that continued scaling of video models represents a promising path towards achieving more capable simulators. These points provide a direct foundation for generating relevant questions that can be understood and answered based on the given information.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What challenges does the model face in simulating interactions such as eating food?',\n",
       "     'How do the authors propose to address the incoherencies and failures of the model in long duration samples?',\n",
       "     'In what way do the capabilities of Sora today suggest a future path for the development of simulators?'],\n",
       "    'question_type': 'open-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': \"The given context describes some of the limitations and challenges faced by Sora in simulating interactions, particularly eating, as well as the occurrence of common failure modes such as incoherencies and spontaneous appearances of objects. It concludes by expressing optimism that scaling video models, as exemplified by Sora, holds promise for the advancement of simulators capable of realistically depicting the physical and digital world and its inhabitants. These questions aim to probe into understanding the specific issues the model confronts, how the authors address such challenges, and the implications of Sora's current capabilities for future developments in simulation technology.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are some common failure modes of the model mentioned?',\n",
       "     'Does scaling video models show promise for creating capable simulators?',\n",
       "     'Who are some of the authors involved in the research?',\n",
       "     'What objectives does the model attempt to simulate?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': 'easy',\n",
       "    'reasoning': 'The context mentions common failure modes like eating food not always yielding correct changes in object state, incoherencies in long duration samples, and spontaneous appearances of objects. The passage implies that scaling video models is a promising path towards developing capable simulators. The authors listed include Tim Brooks, Bill Peebles, et al. The model attempts to simulate the physical and digital world, including objects, animals, and people within it.'}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What are some common failure modes mentioned for the model?',\n",
       "     'What do the authors suggest as a promising path for developing simulators?',\n",
       "     'Who are the authors listed for the research on developing capable simulators?',\n",
       "     'Is scaling of video models considered beneficial for the advancement of simulators?',\n",
       "     'How can one cite the research conducted on simulators of the physical and digital world?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': None,\n",
       "    'reasoning': \"The context mentions specific failure modes such as incoherencies and spontaneous appearances of objects, which are direct answers to the first question. It also highlights that scaling up video models is seen as a promising path for simulator development, answering the second and fourth question. The list of authors provided directly answers the third question. For citing their research, the context includes a specific URL, directly answering the fifth question. These questions are designed to extract detailed knowledge contained within the provided context around the model's limitations, development strategy, authorship, and citation method.\"}},\n",
       "  {'context': {'body': 'Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\xa0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL·E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n',\n",
       "    'id': 3,\n",
       "    'title': None},\n",
       "   'questions': {'questions': ['What model demonstrates the potential of scaling video models for simulating the physical and digital world?',\n",
       "     'Which are some common failure modes identified for the model?',\n",
       "     'Can scaling video models potentially lead to the development of capable simulators for the physical and digital world, and its inhabitants?'],\n",
       "    'question_type': 'closed-ended',\n",
       "    'question_level': None,\n",
       "    'reasoning': \"To generate questions of varying difficulty levels, we analyze the context provided about a model named Sora and its potential in simulating the physical and digital world, including the complexities and challenges, such as certain interactions not yielding correct object state changes and other common failure modes. An easy question would inquire about the model that exhibits this potential (Sora), a medium question would address the specific common failure modes identified, and a hard question would explore the broader potential impact of scaling video models on simulating the physical and digital world and its inhabitants, assessing the reader's comprehension of the model's scope and significance.\"}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = await few_shot_generate_questions(answers, few_shot_examples, response_model=CoTContextQuestions, question_types=['open-ended', 'closed-ended'], question_levels=['easy', 'medium', 'hard'])\n",
    "questions.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Low Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def filter_questions(questions: IterableRagQuestionContext) -> IterableRagQuestionContext:\n",
    "    filtered_list = []\n",
    "    for question_group in questions.question_groups:\n",
    "        context = question_group.context.body\n",
    "        instructions = create_chat_instruction(\n",
    "            system_instruction=\"You are a helpful assistant and a worldclass question master capable of determining if questions are good.\",\n",
    "            user_instruction=f\"Filter out questions that are low quality, are not relevant to the context, or incorrect given the answer. Return a list of booleans determining if the question should be removed. This list should map directly to the question sequence. \\n\\nContext: \\n{context}\\n\\nQuestions: \\n{question_group.questions} \"\n",
    "        )\n",
    "        filtered_questions = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=instructions.dict()['messages'],\n",
    "            response_model=FilterQuestions\n",
    "        )\n",
    "        filtered_list.append(filtered_questions)\n",
    "    filtered_list = await asyncio.gather(*filtered_list)\n",
    "    # keep only questions that are not filtered\n",
    "    filtered_questions = []\n",
    "    for i, (question_group, filtered_group) in enumerate(zip(questions.question_groups, filtered_list)):\n",
    "        filtered_questions.append(RagQuestionContext(\n",
    "            context=question_group.context,\n",
    "            questions=CoTContextQuestions(questions=[question_group.questions.questions[f] for f in range(len(question_group.questions.questions)) if filtered_group.should_remove[f] == False],\n",
    "                question_type=question_group.questions.question_type,\n",
    "                question_level=question_group.questions.question_level,\n",
    "                reasoning=question_group.questions.reasoning\n",
    "            )\n",
    "        ))\n",
    "    return IterableRagQuestionContext(question_groups=filtered_questions)\n",
    "\n",
    "class FilterQuestions(BaseModel):\n",
    "    should_remove: List[bool] = Field(..., description=\"Whether or not to remove questions that are low quality, are not relevant to the context, or incorrect given the answer. Values should match question index\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question_groups\": [\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What is the main purpose of scaling video generation models?\",\n",
      "          \"How does Sora differ from previous generative models of video data?\",\n",
      "          \"Why are videos and images turned into patches for Sora's training?\",\n",
      "          \"How does Sora generate high fidelity videos?\",\n",
      "          \"How does training on videos of variable sizes benefit Sora?\",\n",
      "          \"In what ways can Sora be prompted to generate videos?\",\n",
      "          \"What technique is used to improve language understanding for Sora's video generation?\",\n",
      "          \"How is Sora capable of extending generated videos?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context discusses the development and capabilities of Sora, a video generation model capable of generating high fidelity videos from text and other prompts. The purpose of scaling video generation models is to build general purpose simulators of the physical world. Sora is distinguished from previous models by its ability to generate videos of diverse durations, aspect ratios, and resolutions. It turns videos and images into patches for training, enabling it to handle diverse visual data efficiently. The generation process involves converting raw video into a lower-dimensional latent space and then decompressing it during inference. Training on videos of their native sizes allows for flexibility in sampling and improves video framing and composition. Sora can be prompted both with text and visually, e.g., with images or pre-existing videos, broadening its application scope. Language understanding is enhanced through training on highly descriptive video captions. Lastly, Sora can extend videos both forwards and backwards in time, showcasing its versatility.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What unique representational approach does Sora take for training on diverse types of videos and images?\",\n",
      "          \"How does Sora's training differ from past approaches to video generation?\",\n",
      "          \"In what ways can Sora be prompted to generate videos?\",\n",
      "          \"What impact does training on highly descriptive video captions have on Sora's generated videos?\",\n",
      "          \"How does Sora improve the framing and composition of generated videos compared to models trained on square crops?\",\n",
      "          \"What are some applications of Sora's ability to understand and generate videos based on text?\",\n",
      "          \"How does the capability to generate and extend videos make Sora versatile in video editing tasks?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"medium\",\n",
      "        \"reasoning\": \"The context introduces Sora, a large-scale video generation model with several innovative features. It transforms visual data into a unified representation using spacetime patches, enhancing generative model training on diverse video and image types. Unlike previous models that often standardize video size, Sora trains on data at its native size, allowing flexibility in video duration, resolution, and aspect ratio. It can generate videos based on text prompts or from pre-existing images and videos, applying highly descriptive captions for improved text fidelity and overall quality. Additionally, Sora's method improves framing and composition by training on videos at their natural aspect ratios and enables a range of video editing tasks, such as creating perfectly looping videos or extending videos in time. The questions explore these unique features and their implications for Sora's training approach, generative capabilities, and potential applications.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"How does Sora turn videos into a unified representation for training?\",\n",
      "          \"What advantages does training on data in its native size offer for video generation?\",\n",
      "          \"How does Sora's training on descriptive video captions affect its performance?\",\n",
      "          \"Explain how Sora can generate videos from images.\",\n",
      "          \"What makes the prompting capability of Sora versatile?\",\n",
      "          \"Describe the process and benefits of scaling transformers for video generation according to the text.\",\n",
      "          \"How does Sora compare to models trained on square cropped videos?\",\n",
      "          \"What additional inputs besides text can Sora be prompted with?\",\n",
      "          \"In what ways can Sora extend generated videos?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"medium\",\n",
      "        \"reasoning\": \"These questions cover specific aspects of the technical approach, implementation, and capabilities of Sora, detailed in the context about its video generation modeling. They are meant to deepen understanding of Sora's innovative aspects, including its method for turning visual data into patches, the benefits of training on data at its native size, how training on descriptive captions enhances text fidelity and video quality, the flexibility in generating videos from images, the versatility in prompting with different inputs, and the process of scaling transformers for video generation. Furthermore, these questions explore comparisons with other models and provide insight into Sora's unique ability to extend videos.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What essensial elements enable Sora to train on diverse types of videos and images?\",\n",
      "          \"How does Sora achieve high quality video generation?\",\n",
      "          \"What unique capability allows Sora to perform a variety of editing tasks?\",\n",
      "          \"What approach does Sora use to improve language understanding in video generation?\",\n",
      "          \"How does Sora handle variable video durations, resolutions, and aspect ratios?\",\n",
      "          \"In what ways can you prompt Sora to generate content?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context provides detailed information about the technical aspects and capabilities of Sora, a video generation model. It explains the process of turning videos into patches and how this method allows for handling diverse types of videos and images. The context highlights the innovative use of diffusion transformers for scaling and improving the quality of generated videos. Furthermore, it mentions the ability to prompt Sora with various inputs, including images and pre-existing videos, enabling a wide range of editing tasks. Also, it details the training on highly descriptive video captions to enhance language understanding and fidelity in video generation. Lastly, it describes Sora's flexibility in generating content for different device aspect ratios by training on native-size data, accommodating variable durations, resolutions, and aspect ratios.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"Why do models like Sora turn videos into patches for training?\",\n",
      "          \"What is the purpose of the video compression network in the context of Sora's training?\",\n",
      "          \"How does Sora handle videos and images of variable durations, resolutions, and aspect ratios?\",\n",
      "          \"What advantages does training on videos in their native sizes provide for Sora?\",\n",
      "          \"What method does Sora use to improve language understanding for video generation?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"medium\",\n",
      "        \"reasoning\": \"To understand the comprehensive training process and architecture of Sora for effective video generation. The questions focus on: \\n- The rationale behind the use of patches for video representation.\\n- The role of the video compression network in reducing the dimensionality of visual data.\\n- Sora's capability to process videos and images irrespective of their variable characteristics.\\n- The benefits of utilizing videos' original sizes during training.\\n- Techniques implemented by Sora to enhance the text fidelity and quality of generated videos through improved language understanding.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"\\n\\n\\nVideo generation models as world simulators\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit ResearchVideo generation models as world simulatorsWe explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.February 15, 2024More resourcesView Sora overviewVideo generation,\\u00a0Sora,\\u00a0Milestone,\\u00a0ReleaseThis technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora\\u2019s capabilities and limitations. Model and implementation details are not included in this report.Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data\\u2014it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.Turning visual data into patchesWe take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.[^13][^14] The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text\\u2014code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.[^15][^16][^17][^18] We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,[^19] and subsequently decomposing the representation into spacetime patches.Video compression networkWe train a network that reduces the dimensionality of visual data.[^20] This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.Spacetime latent patchesGiven a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.Scaling transformers for video generationSora is a diffusion model[^21][^22][^23][^24][^25]; given input noisy patches (and conditioning information like text prompts), it\\u2019s trained to predict the original \\u201cclean\\u201d patches. Importantly, Sora is a diffusion transformer.[^26] Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,[^13][^14] computer vision,[^15][^16][^17][^18] and image generation.[^27][^28][^29]In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.Base compute4x compute32x computeVariable durations, resolutions, aspect ratiosPast approaches to image and video generation typically resize, crop or trim videos to a standard size\\u2014e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.Sampling flexibilitySora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution\\u2014all with the same model.Improved framing and compositionWe empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model\\u00a0trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.Language understandingTraining text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL\\u00b7E 3[^30] to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.Similar to DALL\\u00b7E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.{\\n  \\\"key\\\": \\\"characters\\\"\\n} wearing {\\n  \\\"key\\\": \\\"clothings\\\"\\n} taking a pleasant stroll in {\\n  \\\"key\\\": \\\"places\\\"\\n} during {\\n  \\\"key\\\": \\\"weathers\\\"\\n}Prompting with images and videosAll of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks\\u2014creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.Animating DALL\\u00b7E imagesSora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL\\u00b7E 2[^31] and DALL\\u00b7E 3[^30] images.A Shiba Inu dog wearing a beret and black turtleneck.Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.An image of a realistic cloud that spells \\u201cSORA\\u201d.In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.Extending generated videosSora is also capable of extending videos, either forward or backward in time.\",\n",
      "        \"id\": 0,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What enables Sora to train on videos and images of various resolutions, durations, and aspect ratios?\",\n",
      "          \"How does Sora improve the framing and composition of generated videos compared to square-cropped training?\",\n",
      "          \"What benefit does using highly descriptive video captions for training Sora have on the generated videos?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"For the easy question, identifying the feature that allows Sora to handle diverse video and image types is fairly straightforward, given the context's emphasis on the generative model's adaptability. For the medium question, understanding that training on videos at native aspect ratios rather than square crops improves composition and framing requires a bit more insight into how different training data affects the model's output. Lastly, for the hard question, grasping that training with highly descriptive captions enhances the text fidelity and overall quality of the generated videos demands a deeper comprehension of the relationship between textual and visual data in generative models.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What technique allows Sora to transform the styles and environments of videos?\",\n",
      "          \"How can a video be edited to seamlessly loop both forward and backward?\",\n",
      "          \"What can be achieved by using Sora to interpolate between two videos?\",\n",
      "          \"What kind of settings can Sora change in a video?\",\n",
      "          \"Is Sora capable of generating images, besides editing videos?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context mentions using SDEdit to transform the styles and environments of input videos, which answers the first question. For the second question, the context describes a method where videos are extended backward from a segment, making seamless infinite loops possible. The third question is addressed by noting that Sora can interpolate between two videos, creating seamless transitions. Various settings mentioned, like changing a video to be in a lush jungle or a 1920s setting, address the fourth question. Lastly, it is stated that Sora is also capable of generating images, answering the fifth question.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"How can videos be seamlessly extended to create an infinite loop?\",\n",
      "          \"What enables Sora to transform the environments and styles of videos?\",\n",
      "          \"What are some examples of video settings changes enabled by SDEdit for Sora?\",\n",
      "          \"How does Sora achieve transitions between videos with different subjects?\",\n",
      "          \"What additional capability does Sora have besides editing videos?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context describes a technique for extending videos forward and backward to create an infinite loop, mentioning the role of diffusion models and a specific method called SDEdit in altering video environments and styles. Examples of such alterations include changing the video's setting to various imaginative environments. It also mentions Sora's ability to interpolate between two videos for smooth transitions and its capability to generate images, providing a comprehensive overview of Sora's video and image editing functionalities.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What is the purpose of extending a video both forward and backward?\",\n",
      "          \"How does SDEdit transform the styles and environments of input videos?\",\n",
      "          \"What are some examples of how the video setting can be changed using SDEdit?\",\n",
      "          \"What does the technique of gradually interpolating between two input videos achieve?\",\n",
      "          \"How does extending a video in both directions benefit video-to-video editing and diffusion models?\",\n",
      "          \"In what ways can Sora modify the environment of a video?\",\n",
      "          \"Describe the process and benefit of connecting videos through gradual interpolation.\",\n",
      "          \"Explain how the ability to generate images complements Sora's video editing capabilities.\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"The context provides an overview of using diffusion models and techniques like SDEdit for video editing and alterations, particularly focusing on video extension, environment transformation, seamless loops, and blending video scenes. Questions of varying difficulty are derived to explore the principle and application of these methods, the specific changes achievable with Sora, and their underpinning technology or rationale.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What technique allows Sora to transform the style and environment of video inputs zero-shot?\",\n",
      "          \"By extending a video both forward and backward, what effect can be achieved?\",\n",
      "          \"What can Sora do aside from editing videos?\",\n",
      "          \"Can Sora gradually interpolate between two distinct videos to create seamless transitions?\",\n",
      "          \"What is the purpose of extending videos backwards from a specific segment?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"In the context, it is indicated that the technique SDEdit enables Sora to transform the styles and environments of input videos zero-shot. It also mentions that extending a video both forward and backward allows for the creation of a seamless infinite loop. Additionally, Sora has the capability not only for video-to-video editing but also for generating images. Sora can interpolate between two distinct videos to create seamless transitions. Extending videos backwards from a specific segment is part of a technique to produce a seamless infinite loop as mentioned.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"How do diffusion models contribute to video editing?\",\n",
      "          \"What technique allows for the transformation of video styles and environments zero-shot?\",\n",
      "          \"In what way can videos be extended to create a seamless infinite loop?\",\n",
      "          \"What is the result of extending videos backward in time from a generated segment?\",\n",
      "          \"How can videos gradually interpolate between two different scenes or subjects?\",\n",
      "          \"Can Sora generate images as well as edit videos?\",\n",
      "          \"What are some examples of settings Sora can apply to videos?\",\n",
      "          \"Is it possible to maintain specific elements, like color, while changing the video setting with Sora?\",\n",
      "          \"How does extending videos both forward and backward benefit video editing?\",\n",
      "          \"Can Sora transform videos into different animation styles or themes?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context provides a detailed example of how diffusion models, specifically the SDEdit technique, are applied in video editing to change styles and environments of videos zero-shot. It explains the capability to extend videos to create infinite loops and to interpolate between videos for seamless transitions. Additionally, it mentions Sora's ability to generate images, apply various settings and themes to videos, and maintain specific video elements like color during transformations. This makes the basis for creating questions that are easy to answer from the given context.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Below are three videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the three videos starts different from the others, yet all three videos lead to the same ending.00:0000:20We can use this method to extend a video both forward and backward to produce a seamless infinite loop.Video-to-video editingDiffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,[^32] to Sora. This technique enables Sora to transform\\u00a0 the styles and environments of input videos zero-shot.Input videochange the setting to be in a lush junglechange the setting to the 1920s with an old school car. make sure to keep the red colormake it go underwaterchange the video setting to be different than a mountain? perhaps joshua tree?put the video in space with a rainbow roadkeep the video the same but make it be wintermake it in claymation animation stylerecreate in the style of a charcoal drawing, making sure to be black and whitechange the setting to be cyberpunkchange the video to a medieval thememake it have dinosaursrewrite the video in a pixel art styleConnecting videosWe can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.Image generation capabilitiesSora is also capable of generating images.\",\n",
      "        \"id\": 1,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What allows for the creation of an infinite video loop?\",\n",
      "          \"Which technique allows Sora to change the styles and environments of videos?\",\n",
      "          \"How does Sora create transitions between videos with different subjects and scenes?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context describes a method where videos are extended backwards in time from a generated segment, leading to an infinite loop. It also mentions the use of SDEdit to transform the styles and environments of input videos for Sora, and the ability of Sora to interpolate between two input videos to create seamless transitions. These points directly answer the questions proposed, which are made to assess the reader's comprehension of the key points in the text.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What emergent capabilities do video models trained at scale demonstrate?\",\n",
      "          \"How does the model ensure 3D consistency in generated videos?\",\n",
      "          \"What is the challenge in maintaining temporal consistency in video generation systems?\",\n",
      "          \"How can Sora simulate actions that affect the state of the world?\",\n",
      "          \"In what ways can Sora simulate digital worlds like video games?\",\n",
      "          \"What are the limitations of Sora as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context details the emergence of capabilities in Sora, a video generation model, when it's trained at scale. It outlines specific capabilities like 3D consistency, long-range coherence and object permanence, the simulation of actions affecting the world, and the simulation of digital worlds like video games. Moreover, it mentions Sora's limitations, such as not accurately modeling the physics of basic interactions. Each question is aimed at extracting understanding on distinct aspects mentioned in the context, making them unique and relevant to the described features and limitations of Sora.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What capabilities does Sora exhibit when trained at scale?\",\n",
      "          \"How does Sora handle 3D consistency and object permanence in video generation?\",\n",
      "          \"In what ways can Sora interact with and simulate the world?\",\n",
      "          \"What limitations does Sora currently have as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"medium\",\n",
      "        \"reasoning\": \"Analyzing the context reveals Sora's capabilities when trained at scale, such as generating videos with dynamic camera movement, maintaining temporal consistency over long videos, simulating actions that affect the world, and recreating artificial processes like video games. Furthermore, the context details challenges such as accurately modeling the physics of interactions like glass shattering. Therefore, questions are designed to probe understanding of Sora's emergent capabilities at scale, its handling of 3D consistency and object permanence, its interaction with and simulation of the world, and its current limitations, offering a medium-level difficulty that requires comprehension and synthesis of the information provided.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What capabilities does the model Sora exhibit when trained at scale?\",\n",
      "          \"What does the ability to maintain 3D consistency imply about Sora's simulation of videos?\",\n",
      "          \"How does Sora simulate interactions with the world and give an example?\",\n",
      "          \"In what ways are video game worlds simulated by Sora, and provide an example?\",\n",
      "          \"What are the limitations of Sora as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"The context provides detailed descriptions of the emergent capabilities of the model Sora when it is trained at scale. These capabilities include maintaining 3D consistency, long-range coherence and object permanence, interacting with the world, and simulating digital worlds like video games. An example of Sora simulating interactions is a man eating a burger and leaving bite marks. In the context of simulating video games, Sora can control a player in Minecraft while rendering the world and its dynamics. However, the model has its limitations, such as not accurately modeling the physics of certain interactions like glass shattering.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What enables Sora to simulate certain aspects of the physical world?\",\n",
      "          \"What are some emergent capabilities of Sora when trained at scale?\",\n",
      "          \"Can Sora generate images with variable resolutions?\",\n",
      "          \"Is Sora capable of maintaining 3D consistency in videos?\",\n",
      "          \"Does Sora succeed in maintaining long-range coherence and object permanence in videos?\",\n",
      "          \"Can Sora simulate actions that change the world in simple ways, like eating or painting?\",\n",
      "          \"Is Sora able to simulate video games like Minecraft?\",\n",
      "          \"What are some limitations of Sora as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context describes various capabilities and limitations of Sora, an advanced simulation model. It mentions emergent properties like 3D consistency, long-range coherence, and the ability to simulate interactions with the world and video games. These capabilities and limitations can directly form the basis of closed-ended questions that are considered easy because they require identifying specific details mentioned in the passage.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What enables Sora to simulate some aspects of the physical world?\",\n",
      "          \"In terms of video generation, what challenge has Sora managed to partially overcome?\",\n",
      "          \"How does Sora simulate interactions that change the state of the world?\",\n",
      "          \"What kind of digital processes can Sora simulate effectively?\",\n",
      "          \"What limitations does Sora currently face as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"medium\",\n",
      "        \"reasoning\": \"The context explains the capabilities and limitations of Sora in simulating videos that replicate both the physical and digital worlds. These explanations serve as the basis for developing questions around the specifics of its capabilities (like simulating aspects of the physical world, overcoming video generation challenges, simulating interactions with the world, and simulating digital processes) and its limitations. By focusing on these key areas, the questions assess the reader's understanding of Sora's functionalities and the current challenges it faces as a simulator.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes\\u2014up to 2048x2048 resolution.Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of fieldVibrant coral reef teeming with colorful fish and sea creaturesDigital art of a young tiger under an apple tree in a matte painting style with gorgeous detailsA snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2Emerging simulation capabilitiesWe find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.\\u2014they are purely phenomena of scale.3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.Simulating digital worlds. Sora is also able to simulate artificial processes\\u2013one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning \\u201cMinecraft.\\u201dThese capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.DiscussionSora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering.\",\n",
      "        \"id\": 2,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What resolution can the model generate images up to?\",\n",
      "          \"How does Sora maintain three-dimensional space consistency?\",\n",
      "          \"In what way does Sora simulate interactions that affect the state of the world?\",\n",
      "          \"What are some limitations of Sora as a simulator?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The answer to the first question about the maximum image resolution (2048x2048) is explicitly stated in the context, which should be easy for readers to find. The answer to the second question on maintaining three-dimensional space consistency through dynamic camera motion is also directly mentioned, making it straightforward. The third question queries how Sora simulates interactions like leaving bite marks or painting strokes, which is described in the context and should be easily understood. Lastly, the fourth question about Sora's limitations, such as not accurately modeling the physics of interactions like glass shattering, is addressed directly in the context, thus fitting the easy category.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What challenges do video models face in simulating the physical and digital world?\",\n",
      "          \"Who are the authors of the study on the development of simulators using video models?\",\n",
      "          \"What does the capability of Sora indicate about the future of video model scaling?\",\n",
      "          \"Where can the citation for this study be found?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context discusses the capabilities of a video model named Sora and the challenges it faces, such as incorrect changes in object state and spontaneous appearances of objects. It highlights the significance of scaling video models for simulating the physical and digital world. The authors of the study are listed, and the context provides a link for citation, making these elements relevant for generating questions that are easy and open-ended.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What are some common failure modes of the model mentioned?\",\n",
      "          \"How does the capabilities of Sora contribute to the development of simulators?\",\n",
      "          \"What is the promising path towards developing capable simulators of the physical and digital world according to the authors?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context mentions specific failure modes of the model, highlights the role of Sora's capabilities in advancing the development of simulators, and suggests that continued scaling of video models represents a promising path towards achieving more capable simulators. These points provide a direct foundation for generating relevant questions that can be understood and answered based on the given information.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What challenges does the model face in simulating interactions such as eating food?\",\n",
      "          \"How do the authors propose to address the incoherencies and failures of the model in long duration samples?\",\n",
      "          \"In what way do the capabilities of Sora today suggest a future path for the development of simulators?\"\n",
      "        ],\n",
      "        \"question_type\": \"open-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The given context describes some of the limitations and challenges faced by Sora in simulating interactions, particularly eating, as well as the occurrence of common failure modes such as incoherencies and spontaneous appearances of objects. It concludes by expressing optimism that scaling video models, as exemplified by Sora, holds promise for the advancement of simulators capable of realistically depicting the physical and digital world and its inhabitants. These questions aim to probe into understanding the specific issues the model confronts, how the authors address such challenges, and the implications of Sora's current capabilities for future developments in simulation technology.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What are some common failure modes of the model mentioned?\",\n",
      "          \"Does scaling video models show promise for creating capable simulators?\",\n",
      "          \"Who are some of the authors involved in the research?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": \"easy\",\n",
      "        \"reasoning\": \"The context mentions common failure modes like eating food not always yielding correct changes in object state, incoherencies in long duration samples, and spontaneous appearances of objects. The passage implies that scaling video models is a promising path towards developing capable simulators. The authors listed include Tim Brooks, Bill Peebles, et al. The model attempts to simulate the physical and digital world, including objects, animals, and people within it.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What are some common failure modes mentioned for the model?\",\n",
      "          \"What do the authors suggest as a promising path for developing simulators?\",\n",
      "          \"Who are the authors listed for the research on developing capable simulators?\",\n",
      "          \"Is scaling of video models considered beneficial for the advancement of simulators?\",\n",
      "          \"How can one cite the research conducted on simulators of the physical and digital world?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"The context mentions specific failure modes such as incoherencies and spontaneous appearances of objects, which are direct answers to the first question. It also highlights that scaling up video models is seen as a promising path for simulator development, answering the second and fourth question. The list of authors provided directly answers the third question. For citing their research, the context includes a specific URL, directly answering the fifth question. These questions are designed to extract detailed knowledge contained within the provided context around the model's limitations, development strategy, authorship, and citation method.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"context\": {\n",
      "        \"body\": \"Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model\\u2014such as incoherencies that develop in long duration samples or spontaneous appearances of objects\\u2014in our landing page.We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.AuthorsTim BrooksBill PeeblesConnor HolmesWill DePueYufei GuoLi JingDavid SchnurrJoe TaylorTroy LuhmanEric LuhmanClarence NgRicky WangAditya RameshAcknowledgmentsCitationPlease cite as Brooks, Peebles, et al., and use the following BibTeX for citation:\\u00a0https://openai.com/bibtex/videoworldsimulators2024.bibResearchOverviewIndexGPT-4DALL\\u00b7E 3SoraAPIOverviewPricingDocsChatGPTOverviewTeamEnterprisePricingTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI \\u00a9 2015\\u200a\\u2013\\u200a2024Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n\",\n",
      "        \"id\": 3,\n",
      "        \"title\": null\n",
      "      },\n",
      "      \"questions\": {\n",
      "        \"questions\": [\n",
      "          \"What model demonstrates the potential of scaling video models for simulating the physical and digital world?\",\n",
      "          \"Which are some common failure modes identified for the model?\",\n",
      "          \"Can scaling video models potentially lead to the development of capable simulators for the physical and digital world, and its inhabitants?\"\n",
      "        ],\n",
      "        \"question_type\": \"closed-ended\",\n",
      "        \"question_level\": null,\n",
      "        \"reasoning\": \"To generate questions of varying difficulty levels, we analyze the context provided about a model named Sora and its potential in simulating the physical and digital world, including the complexities and challenges, such as certain interactions not yielding correct object state changes and other common failure modes. An easy question would inquire about the model that exhibits this potential (Sora), a medium question would address the specific common failure modes identified, and a hard question would explore the broader potential impact of scaling video models on simulating the physical and digital world and its inhabitants, assessing the reader's comprehension of the model's scope and significance.\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "filtered_questions = await filter_questions(questions)\n",
    "print(json.dumps(filtered_questions.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 questions were removed\n"
     ]
    }
   ],
   "source": [
    "def count_questions(questions: IterableRagQuestionContext) -> int:\n",
    "    return sum([len(q.questions.questions) for q in questions.question_groups])\n",
    "\n",
    "print(f'{count_questions(questions) - count_questions(filtered_questions)} questions were removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "- Add validation to make sure all fields are correct\n",
    "- Adjust prompts and openai parameters to the task\n",
    "- Add reasoning chains to improve quality and diversity of the dataset\n",
    "- Ask for variations of the same question, but be sure to remove data points that are too similiar\n",
    "- Use natural data as seed data and if possible use it in training\n",
    "- Add stronger filtering to remove low quality samples\n",
    "- Scale up the data flywheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
